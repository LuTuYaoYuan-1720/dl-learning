{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c1332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee1b813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4137,  0.2021,  0.0584,  0.2482,  0.0942,  0.1700,  0.0306,  0.2328,\n",
       "          0.0599,  0.1214],\n",
       "        [ 0.2796, -0.0109, -0.0038,  0.2739,  0.1104,  0.1726, -0.0668,  0.2318,\n",
       "         -0.0871,  0.0676]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d82e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204c598",
   "metadata": {},
   "source": [
    "### 自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56012ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # 必须重写forward，实现各个层之间的连接关系，\n",
    "        # 只要定义了forward函数，backward函数会被自动实现\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1fef3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2291,  0.1552,  0.1471,  0.0059, -0.3268,  0.0794, -0.0952, -0.4270,\n",
       "          0.1956, -0.2151],\n",
       "        [-0.0112,  0.1319,  0.0930,  0.0300, -0.2132,  0.0519, -0.0343, -0.3415,\n",
       "          0.1546, -0.1686]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a079f6c3",
   "metadata": {},
   "source": [
    "### 顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a31ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySquential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        # _modules 里面存着 Module 的实例，为一个OrderDict\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[str(idx)] = module\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)       \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d367cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2693, -0.2171, -0.0084, -0.0377, -0.1736,  0.0932,  0.0772, -0.1582,\n",
       "         -0.0167, -0.0280],\n",
       "        [-0.1433, -0.0832,  0.0818, -0.0546, -0.0189,  0.1212,  0.1298, -0.1778,\n",
       "          0.1196,  0.0428]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySquential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73cf2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随即权重参数， 在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc730d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1550, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4ad07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a470df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2038, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a51ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "class BigBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net1 = nn.Linear(20,60)\n",
    "        self.net2 = nn.Linear(20,40)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X1 = self.net1(X)\n",
    "        X2 = self.net2(X)\n",
    "        X3 = torch.cat((X1, X2), 1)\n",
    "        net = nn.Linear(X3.shape[1], 22)\n",
    "        return net(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3fb234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 22])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = BigBlock()\n",
    "net(X).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
